%!TEX root = nextndnvideo-tr.tex
\section{implementation} % (fold)
\label{sec:implementation}
NDNlive and NDNtube are developed using Consumer / Producer API and Gstreamer 1.4.3 library.\footnote{Other versions of Gstreamer may not be compatible}
The supported platforms are Mac OS X and Linux Ubuntu.\footnote{Other Linux platforms are potentially supported} 

\subsection{NDNlive}
As shown in Figure~\ref{fig:ndnlive_arch}, NDNlive consists of two applications --- one is running at the publisher's host and another one at the consumer's host. In this section, we go over implementation details of the publisher's application and then continue with consumer's application.

\begin{figure*}%[htbp]
  \centering
  \includegraphics[scale=0.3]{ndnlive_naming_pro}
  % \vspace{-0.3cm}
  \caption{Locations of producers and consumers in the NDNlive namespace.}
  \label{fig:ndnlive_cp}
  %\vspace{-0.2cm}
\end{figure*}

\subsubsection{Publisher}
\label{ssub:ndnlive_pro}
Publisher's application has four producers: video content producer, video stream information producer, audio content producer and audio stream information producer. Figure~\ref{fig:ndnlive_cp} shows the locations of the producers in the NDNlive namespace. 

Two content\_producers continuously publish video and audio frames by incrementally increasing the correpsonding frame numbers (Figure~\ref{fig:ndnlive_naming}). 

Two stream\_info producers continuously publish up-to-date information about the live streaming media: current frame number, frame rate, video width and height, encoding format (Figure~\ref{fig:ndnlive_naming}).

\paragraph{Negative Acknowledgement} % (fold)
\label{par:negative_acknowledgement}
\vspace{0.3cm}
There are two situations we should consider carefully. 
\begin{enumerate}
	\item The first one is that, because once the consumer started consuming frames, it will have no idea the about the current frame number which producer is producing now. The consumer may sometimes request for a frame number ahead of the producing. It is the producer's duty to inform the consumer of such knowledge. We introduce \textbf{NACK}(\textit{Negative Acknowledgment}) to handle such situation. 

	We can see from Algorithm~\ref{alg:liveproducer}, when the Interest asks for a piece of data not existed (out of date or not be produced yet), this will trigger the \textit{cache\_miss} callback function (\textit{Process\_Interest}). In that function, if the data was not produced (\textit{not\_ready}), the producer will set up an \textit{APPLICATION\_NACK} with \textit{PRODUCER\_ \\\ DELAY} option for this Interest together with the estimated delay time.

	\item At the same time, although before the consumer starts to consume frames, it will ask for the current number, such information may also go out of date because of the network delay. These out-of-date frames will never be produced again, because the streaming is live. When faced with such situation, the producer will simply send a \textbf{NACK} with \textit{NO-DATA} option.
\end{enumerate}
% paragraph negative_acknowledgement (end)

The pseudocode of  NDNlive publisher application is provided in Algorithm~\ref{alg:liveproducer}.

\begin{algorithm}[ht]
\caption{NDNLive producer}
\label{alg:liveproducer}
\begin{algorithmic}[1]
\State $h_v \leftarrow $ \textbf{producer}(/ndn/ucla/ndnlive/stream-1/video/ \\\ content)
\State \textbf{setcontextopt}($h_v$, \textbf{cache\_miss}, \textit{ProcessInterest})
\State \textbf{attach}($h_v$)
\vspace{0.2cm}
	\While{\textit{TRUE}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State $content_v \leftarrow $ video frame captured from camera
	\State \textbf{produce}($h_v$, $Name\textbf{ }suffix_v$, $content_v$)
	\EndWhile
\vspace{0.2cm}
\vspace{0.2cm}
\State $h_a \leftarrow $ \textbf{producer}(/ndn/ucla/ndnlive/stream-1/audio/ \\\  content)
\State \textbf{setcontextopt}($h_a$, \textbf{cache\_miss}, \textit{ProcessInterest})
\State \textbf{attach}($h_a$)
\vspace{0.2cm}
	\While{\textit{TRUE}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State $content_a \leftarrow $ audio frame captured from mirophone
	\State \textbf{produce}($h_a$, $Name\textbf{ }suffix_a$, $content_a$)
	\EndWhile
\vspace{0.4cm}
\Function{ProcessInterest}{Producer \textbf{h}, Interest \textbf{i}}
  \If{\textit{NOT Ready}}
    \State $appNack \leftarrow $ \textbf{AppNack}($i$, \textbf{RETRY-AFTER})
    \State \textbf{setdelay}($appNack$, $estimated\_time$)
    \State \textbf{nack}($h$, $appNack$)
  \EndIf
   \If{\textit{Out of Date}}
    \State $appNack \leftarrow $ \textbf{AppNack}($i$, \textbf{NO-DATA})
    \State \textbf{nack}($h$, $appNack$)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[hbt]
\caption{NDNlive consumer}
\label{alg:liveconsumer}
\begin{algorithmic}[2]
\State $h_v \leftarrow $ \textbf{consumer}(/ndn/ucla/ndnlive//stream-1/video/\\\ content, \textit{UDR})
%\State \textbf{setcontextopt}($h_v$, \textit{EMBEDDED\_MANIFESTS}, \textit{TRUE})
%\State \textbf{setcontextopt}($h_v$, \textbf{receive\_buffer\_size}, 1MB)
\State \textbf{setcontextopt}($h_v$, \textbf{new\_segment}, \textit{ReassambleVideo})
\vspace{0.2cm}
	\While{\textit{reaching Video\_Interval}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State \textbf{consume}($h_v$, $Name\textbf{ }suffix_v$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ReassembleVideo}{Data \textbf{segment}}
    \State $content \leftarrow $ reassemble \textbf{segment}
    \If{\textit{Final\_Segment}}
		\State $video \leftarrow $ decode \textbf{content}
	   	\State Play $video$
	\EndIf
\EndFunction

\vspace{0.4cm}

\State $h_a \leftarrow $ \textbf{consumer}(/ndn/ucla/ndnlive/stream-1/audio/\\\ content, \textit{SDR})
\State \textbf{setcontextopt}($h_a$, \textbf{new\_content}, \textit{ProcessAudio})
\vspace{0.2cm}
	\While{\textit{reaching Audio\_Interval}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State \textbf{consume}($h_a$, $Name\textbf{ }suffix_a$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ReassembleAudio}{Data \textbf{content}}
%   \State $video \leftarrow $ decode \textbf{content}
   	\State $audio \leftarrow $ decode \textbf{content}
   	\State Play $audio$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Consumer}
\label{ssub:ndnlive_con}
NDNlive consumer must fetch the live stream information to set up the Gstreamer playing pipeline before it can request any of the audio or video frames. The application has four consumers: video content consumer, video stream information consumer, audio content consumer and audio stream information consumer. 

\paragraph{Data retrieval}
\label{par:ndnlive_dataretrievalprotocol}
\vspace{0.3cm}
Consumer / Producer API protocol suite offers three data retrieval protocols: SDR, UDR, RDR. In this section we describe how NDNlive consumer application uses SDR and UDR protocols. The pseudocode of the NDNlive consumer application is provided in the Algorithm~\ref{alg:liveconsumer}.

\begin{enumerate}
	\item {\textit{Content Retrieval}}
	
In the case of the live media streaming, the consumer application must continue retrieving video and audio frames at all times in order to keep up with the data production rate. All segments of each frame must be retrieved as fast as possible and the fetching process should not block other frames because of segment losses. 
	
NDNlive video content consumer uses \textbf{UDR} (\textit{Unreliable Data Retrieval}) protocol for video frame retrieval. Since \textbf{UDR} pipelines Interests transmission and does not provide ordering, some Data segments may arrive out of order. NDNlive consumer application takes care of Data segment reassembly and drops the whole frame is any of its segments are lost. 

NDNlive audio content consumer uses \textbf{SDR} (\textit{Simple Data Retrieval}) for audio frame retrieval. UDR does not pipeline Interest packets, which satisfies our requirements, since the audio frame is small enough to fit in just one Data segment.

	\item {\textit{Stream Information Retrieval}} % (fold)
	
Stream information is periodically updated by the video publisher, which essentially means creation of a new Data packet with a unique name (e.g. new timestamp name component). The consumer that is trying to join the live stream does not know the unique name of the latest stream information object, and therefore cannot use UDR or RDR protocols which assume such knowledge. A simple solution of this problem is to use  \textbf{SDR} (\textit{Simple Data Retrieval}) protocol with \textit{Right\_Most\_Child} option set as TRUE. The protocol generates a single Interest packet with \textit{RightmostChildSelector} which is capable of fetching the latest stream info object. 

\end{enumerate}

\paragraph{Frame-to-frame interval} % (fold)
\label{par:consume_interval}
\vspace{0.3cm}
Consumer application should control the Interest sending speed. If it sends Interests too aggressively and the data is not yet produced by the publisher application, the playback may collapse. If it sends Interests too slowly, the playback may fall behind the video generation. NDNlive uses constant frame rate encoding, therefore for a video, which is encoded by 30 frames per second, the interval between frames is $1000/30 \approx {33.3}$ millisecond. In other words, every 33 milliseconds the application calls \textit{consume()} that attempts to fetch all segments of the frame as quickly as possible.

%\subsubsection{Some other vital parts}
%\paragraph{Signing and Verification} % (fold)
%\label{par:signing_and_verification}
%\vspace{0.3cm}
%Every NDN package should be signed with the producer's private key, only the verified frame can be retrieved successfully. But signing and verification are very time consuming. Consumer / Producer uses \textit{Manifest} \cite{api-tr} to improve the signing and verification performance. 

%Instead of signing every segment in one frame, the producer only needs signing and verifying the Manifest. This option can be easily turned on or off by set \textit{EMBEDED\_\\\ MANIFEST} as TRUE or FALSE.
% paragraph signing_and_verification (end){Signing and Verification}

\paragraph{Synchronization of video and audio}
\label{par:sync}
\vspace{0.3cm}
Since NDNlive is streaming video and audio separately, it is a vital problem to keep these streams synced. When video and audio frames are captured, they are timestamped by the Gstreamer. The time information is recorded in \textit{GstBuffer} data structure containing the media data, and transferred along with every video or audio frame. When the consumer fetches the video or audio frames separately, the video and audio frames are pushed into the same \textit{GstQueue}. Gstreamer extracts the timestamps present in the video and audio frames, and displays the content in synchronized mode. 



\subsection{NDNtube}
\begin{figure*}[ht]
  \centering
  \includegraphics[scale=0.3]{ndntube_naming_pro}
  % \vspace{-0.3cm}
  \caption{Locations of producers and consumers in the NDNtube namespace.}
  \label{fig:ndntube_cp}
  %\vspace{-0.2cm}
\end{figure*}
Although the namespace of NDNtube might look very similar to the namespace of NDNlive, the patterns of the data production and retrieval are quite different. %This section provides thWe will describe them in detail in this section.

\begin{algorithm}[ht]
\caption{NDNTube publisher}
\label{alg:recordproducer}
\begin{algorithmic}[3]
\State $h_v \leftarrow $ \textbf{producer}(/ndn/ucla/ndntube/video-1234/ \\\ video)
\State \textbf{setcontextopt}($h_v$, \textbf{local\_repo}, \textit{TRUE})
\vspace{0.2cm}
	\While{\textit{NOT Final\_Frame}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State $content_v \leftarrow $ video frame
	\State \textbf{produce}($h_v$, $Name\textbf{ }suffix_v$, $content_v$)
	%\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}
\vspace{0.2cm}
\State $h_a \leftarrow $ \textbf{producer}(/ndn/ucla/ndntube/video-1234/ \\\ audio)
\State \textbf{setcontextopt}($h_a$, \textbf{local\_repo}, \textit{TRUE})
\vspace{0.2cm}
	\While{\textit{NOT Final\_Frame}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State $content_a \leftarrow $ audio frame
	\State \textbf{produce}($h_a$, $Name\textbf{ }suffix_a$, $content_a$)
	%\State $framenumber ++$
	\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{NDNTube consumer}
\label{alg:recordconsumer}
\begin{algorithmic}[4]
\State $h_v \leftarrow $ \textbf{consumer}(/ndn/ucla/ndntube/video-1234/ \\\ video, \textit{RDR})
%\State \textbf{setcontextopt}($h_v$, \textit{EMBEDDED\_MANIFESTS}, \textit{TRUE})
%\State \textbf{setcontextopt}($h_v$, \textbf{receive\_buffer\_size}, 1MB)
\State \textbf{setcontextopt}($h_v$, \textbf{new\_content}, \textit{ProcessVideo})
\vspace{0.2cm}
	\While{\textit{NOT Final\_Frame}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State \textbf{consume}($h_v$, $Name\textbf{ }suffix_v$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ProcessVideo}{byte[] \textbf{content}}
   \State $video \leftarrow $ decode \textbf{content}
%   \State $audio \leftarrow $ decode \textbf{content}
   \State Play $video$
\EndFunction

\vspace{0.4cm}

\State $h_a \leftarrow $ \textbf{consumer}(ndn/ucla/ndntube/video-1234/ \\\ audio, \textit{RDR})
\State \textbf{setcontextopt}($h_a$, \textbf{new\_content}, \textit{ProcessAudio})
\vspace{0.2cm}
	\While{\textit{NOT Final\_Frame}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State \textbf{consume}($h_a$, $Name\textbf{ }suffix_a$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ProcessAudio}{byte[] \textbf{content}}
%   \State $video \leftarrow $ decode \textbf{content}
   	\State $audio \leftarrow $ decode \textbf{content}
   	\State Play $audio$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Publisher}
Publisher's application has three producers: dynamic playlist producer, video content producer and audio content producer. Figure~\ref{fig:ndntube_cp} shows the locations of the producers in the NDNtube namespace.

Playlist producer P$_{\text{1}}$ is responsible for generating the latest playlist every time a video file is added or removed from the collection of media resources. Producer P$_{\text{1}}$ runs as long as the whole publisher application.

Video content producer P$_{\text{2}}$ is responsible for publishing video frames and the stream information object for a each particular media resource. Since producer P$_{\text{2}}$ is configured with \textit{LOCAL\_REPO} option, all packets are written to the repo running on the same local host. After all video frames as well as stream information object are successfully inserted in the repo, producer P$_{\text{2}}$ terminates its execution.\footnote{Publisher process continues to run.} 

Audio content producer P$_{\text{3}}$ is responsible for publishing audio frames and the stream information object for a each particular media resource. Since producer P$_{\text{3}}$ is configured with \textit{LOCAL\_REPO} option, all packets are written to the repo running on the same local host. After all audio frames as well as stream information object are successfully inserted in the repo, producer P$_{\text{3}}$ terminates its execution (Preudocode~\ref{alg:recordproducer}).

\subsubsection{Consumer}

The application has five consumers: playlist consumer C$_{\text{1}}$, video content consumer C$_{\text{2}}$, video stream information consumer C$_{\text{3}}$, audio content consumer C$_{\text{4}}$ and audio stream information consumer C$_{\text{5}}$. %Figure~\ref{fig:ndntube_cp} shows the locations of the consumers in the NDNtube namespace.


\paragraph{Data retrieval} % (fold)
\label{par:ndntube_data_retrieval}
\vspace{0.3cm}
% paragraph data_retrieval (end)

In this section we describe how NDNtube consumer application uses SDR and RDR protocols. The pseudocode of the NDNtube consumer application is provided in the Algorithm~\ref{alg:recordconsumer}.

\begin{enumerate}
	\item {\textit{Content Retrieval}}
All video and audio frames as well as stream information objects are retrieved by \textbf{RDR} (\textit{Reliable Data Retrieval}) protocol, which provides ordered and reliable fetching of Data segments. NDNtube video player does not consume a live streaming media, and consequently can afford much larger buffering delays in order to preserve the original quality of the video and audio resources. By default, NDNtube buffers for at least two seconds of video and audio frames of real playback time before it begins (or resumes) its playback. Buffering allows to soften the delays of frame retrieval due to possible Interest retransmissions done by the RDR protocol. 

An expected but nevertheless interesting effect of frame-by-frame reliable delivery shows itself in rare cases when a particular video or audio frame cannot be retrieved within a reasonable amount of time (e.g. Interest retransmissions) and application faces the choice whether it wants to skip the frame or try to consume() it again. Since our goal was to prototype a Youtube-like user experience, in this situation, NDNtube consumer will try to retrieve the same frame again. 

	\item {\textit{Playlist Retrieval}} 
	
Playlist is periodically updated by the video publisher, which essentially means creation of a new Data packet with a unique name (e.g. new timestamp name component). The consumer that is trying to obtain the names of available media resources does not know the unique name of the latest playlist, and therefore cannot use UDR or RDR protocols which assume such knowledge. A simple solution of this problem is to use  \textbf{SDR} (\textit{Simple Data Retrieval}) protocol with \textit{Right\_Most\_Child} option set as TRUE. The protocol generates a single Interest packet with \textit{RightmostChildSelector} which is capable of fetching the latest playlist. 

\end{enumerate}

\paragraph{Frame-to-frame interval} 

Since all the content and stream information already exists in the Repo for a long time, consumer can be quite aggressive with fetching video and audio frames. By default, NDNtube player starts fetching (via consume()) the next frame right after the current was successfully retrieved, which corresponds to the frame-to-frame interval of 0 milliseconds. Having below 0 ms. frame-to-frame interval is also a reality, because it is possible to fetch multiple frames in parallel.

%\paragraph{Synchronization of video and audio} % (fold)
%\label{par:synchronization_between_video_and_audio}
%\vspace{0.3cm}
%There also exists the synchronization problem between video and audio. As we describe above~\ref{par:sync}, the Gstreamer will handle the synchronization part as long as we give the video and audio frame correct timestamps. In NDNLive, it is the capturing component who stamps the frames. In NDNTube, it is the \textit{Dumxer} who is responsible for time stamping. Once the media data flows through \textit{Dumxer}, this component will separate the video stream and audio stream according to their file type such as \textit{MP4} and adding the time information in each \textit{GstBuffer}.

  

The NDNTube consumer's Pseudocode is shown as Algorithm~\ref{alg:recordconsumer}.
% paragraph synchronization_between_video_and_audio (end)

% section implementation (end)