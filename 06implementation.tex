%!TEX root = nextndnvideo-tr.tex
\section{implementation} % (fold)
\label{sec:implementation}
NDNLive and NDNTue are both developed using Consumer / Producer API. This API is an modification version of ndn-cxx library and requires NFD running to forward interests. To compact with Consumer / Producer API and NFD, the project is also written in C++. We use Gstreamer 1.4.3 (other branch not tested) to process media. The supporting platform is UNIX-Like such as Mac OS and Linux. We will explain the implementation details about NDNLive and NDNTube respectively.

\subsection{NDNLive}
As we describe above (Figure~\ref{fig:ndnlive_arch}), the whole implementation is divided into producer host and consumer host. We will introduce the implementation details of each side, then describe some other vital parts we should pay attention to, such as \textit{Signing and Verification, Synchronization}. 
\begin{figure*}%[htbp]
  \centering
  \includegraphics[scale=0.3]{ndnlive_naming_pro}
  % \vspace{-0.3cm}
  \caption{NDNLive Producer and Consumer Structure}
  \label{fig:ndnlive_cp}
  %\vspace{-0.2cm}
\end{figure*}

\subsubsection{Producer}
\label{ssub:ndnlive_pro}
Four producers are presented in producer host (Figure~\ref{fig:ndnlive_cp}): video content producer, video stream information producer, audio content producer and audio stream information producer. 

The content\_producer keeps producing frames with frame number increasing incrementally (Figure~\ref{fig:ndnlive_naming})and publish them to the NDN Network. 

The stream\_info producer aims to provide the information about the live streaming such as frame rate, width, height, stream format. What's more, to help the consumer to catch up with the latest frame, the current frame number should also be included. To distinguish the obsolete stream\_info, timestamp will be appended at the end of name (Figure~\ref{fig:ndnlive_naming}).

\paragraph{Negative Acknowledgement} % (fold)
\label{par:negative_acknowledgement}
There are two situations we should consider carefully. 
\begin{enumerate}
	\item The first one is that, because once the consumer started consuming frames, it will have no idea the about the current frame number which producer is producing. The consumer may sometimes request for a frame number ahead of the producing. It is the producer's duty to inform the consumer about such knowledge. We introduce \textbf{NACK}(\textit{Negative Acknowledgment}) to handle such situation. 

	For example, in Algorithm~\ref{alg:liveproducer} , when the interest asks for a piece of data not existed (out of date or not be produced yet), this will trigger the \textit{cache\_miss} callback function (\textit{Process\_Interest}). In that function, if the data was not produced (\textit{not\_ready}), the producer will set up an \textit{APPLICATION\_NACK} with \textit{PRODUCER\_DELAY} option for this interest together with the estimated delay time.

	\item At the same time, although before the consumer starts to consume frames, it will ask for the current number, such information may also go out of date because of the network delay. These out-of-date frames will never be produced again, because the streaming is live. When faced with such situation, the producer will simply send a \textbf{NACK} with \textit{NO-DATA} option.
\end{enumerate}
% paragraph negative_acknowledgement (end)

\begin{algorithm}[ht]
\caption{NDNLive producer}
\label{alg:liveproducer}
\begin{algorithmic}[1]
\State $h_v \leftarrow $ \textbf{producer}(/ndn/ucla/ndnlive/stream-1/video/ \\\ content)
\State \textbf{setcontextopt}($h_v$, \textbf{cache\_miss}, \textit{ProcessInterest})
\State \textbf{attach}($h_v$)
\vspace{0.2cm}
	\While{\textit{TRUE}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State $content_v \leftarrow $ video frame captured from Camera
	\State \textbf{produce}($h_v$, $Name\textbf{ }suffix_v$, $content_v$)
	\EndWhile
\vspace{0.2cm}
\vspace{0.2cm}
\State $h_a \leftarrow $ \textbf{producer}(/ndn/ucla/ndnlive/stream-1/audio/ \\\  content)
\State \textbf{setcontextopt}($h_a$, \textbf{cache\_miss}, \textit{ProcessInterest})
\State \textbf{attach}($h_a$)
\vspace{0.2cm}
	\While{\textit{TRUE}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State $content_a \leftarrow $ audio frame captured from mirophone
	\State \textbf{produce}($h_a$, $Name\textbf{ }suffix_a$, $content_a$)
	\EndWhile
\vspace{0.4cm}
\Function{ProcessInterest}{Producer \textbf{h}, Interest \textbf{i}}
  \If{\textit{NOT Ready}}
    \State $appNack \leftarrow $ \textbf{AppNack}($i$, \textbf{RETRY-AFTER})
    \State \textbf{setdelay}($appNack$, $estimated\_time$)
    \State \textbf{nack}($h$, $appNack$)
  \EndIf
   \If{\textit{Out of Date}}
    \State $appNack \leftarrow $ \textbf{AppNack}($i$, \textbf{NO-DATA})
    \State \textbf{nack}($h$, $appNack$)
  \EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[hbt]
\caption{NDNLive consumer}
\label{alg:liveconsumer}
\begin{algorithmic}[2]
\State $h_v \leftarrow $ \textbf{consumer}(/ndn/ucla/ndnlive//stream-1/video/\\\ content, \textit{UDR})
%\State \textbf{setcontextopt}($h_v$, \textit{EMBEDDED\_MANIFESTS}, \textit{TRUE})
%\State \textbf{setcontextopt}($h_v$, \textbf{receive\_buffer\_size}, 1MB)
\State \textbf{setcontextopt}($h_v$, \textbf{new\_segment}, \textit{ReassambleVideo})
\vspace{0.2cm}
	\While{\textit{reach Consume\_Interval\_Video}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State \textbf{consume}($h_v$, $Name\textbf{ }suffix_v$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ReassambleVideo}{Data \textbf{segment}}
    \State $content \leftarrow $ reassamble \textbf{segment}
    \If{\textit{Final\_Segment}}
		\State $video \leftarrow $ decode \textbf{content}
	   	\State Play $video$
	\EndIf
\EndFunction

\vspace{0.4cm}

\State $h_a \leftarrow $ \textbf{consumer}(/ndn/ucla/ndnlive/stream-1/audio/\\\ content, \textit{SDR})
\State \textbf{setcontextopt}($h_a$, \textbf{new\_content}, \textit{ProcessAudio})
\vspace{0.2cm}
	\While{\textit{reach Consume\_Interval\_Audio}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State \textbf{consume}($h_a$, $Name\textbf{ }suffix_a$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ReassambleAudio}{Data \textbf{content}}
%   \State $video \leftarrow $ decode \textbf{content}
   	\State $audio \leftarrow $ decode \textbf{content}
   	\State Play $audio$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Consumer}
\label{ssub:ndnlive_con}
Before the consumer asks for the true video data, it must fetch the live stream information to set up the Gstreamer playing pipeline. There are four consumers: video content consumer, video stream information consumer, audio content consumer and audio stream information consumer. 

\paragraph{Data Retrieval Protocol}
There are three Data Retrieval Protocols in Consumer / Producer API : \textbf{SDR, UDR, RDR}. We will illustrate which protocol we used for each consumer.
\begin{enumerate}
	\item {\textit{Content Retrieval}}
	
	Considering about the live streaming situation, the consumer part needs to keep the video and audio retrieving progress running all the time. The aim is to fetch all the segments inside one frame as soon as possible. The fetching process should NOT be blocked because of one segment missing. So we use \textbf{UDR} (\textit{Unreliable Data Retrieval}) for frames retrieval of living streaming. Because the \textbf{UDR} will pipeline the Interests sending, and the segments may received out of order, then the consumer part should take care of the segments reassemble and ordering stuff. If one segment of frame is not retrieved on time, then the whole frame will be skipped. But for audio, the size of audio frame is small enough to fill in one segment, so we would use \textbf{SDR} (\textit{Simple Data Retrieval}) for audio retrieval.

	\item {\textit{Stream Information Retrieval}} % (fold)
	
	Because the stream information contains only one segment and will be fetched only one time (at the beginning of the playing back). We use \textbf{SDR} (\textit{Simple Data Retrieval}) to fetch the stream info for video and audio. Except for the basic stream information, the consumer also needs to obtain the current frame number the producer just produced. So that the frame consumer can start from this frame number and increase it one by one. To retrieve the latest stream information, \textit{Right\_Most\_Child} option should be set as TRUE (Algorithm ~\ref{alg:liveconsumer}).
\end{enumerate}

\paragraph{Consume Interval} % (fold)
\label{par:consume_interval}
In consumer part, we should control the Interest sending speed. If we send them too aggressively, the data in producer side may not get ready. If we send them too slowly, the playing back may not match the video generating speed. Our solution is to send Interests according to the video and audio frame rate. For example, the video frame rate is 30 frame/second, then the \textit{consume\_interval} should be $1000/30 \approx {33.3}$ millisecond. The consume function should be called every \textit{consume\_interval}.
The boost scheduler will schedule the consume process every video or audio interval according to the video or audio \textit{consume\_interval}. 
% paragraph consume_interval (end)
\subsubsection{Some other vital parts}
\paragraph{Signing and Verification} % (fold)
\label{par:signing_and_verification}
Every NDN package should be signed with the producer's private key, only the verified frame can be retrieved successfully. But signing and verification are very time consuming. Consumer / Producer uses \textit{Manifest} \cite{api-tr} to improve the signing and verification performance. 

Instead of signing every segment in one frame, the producer only needs signing and verifying the Manifest. This option can be easily turned on or off by set \textit{EMBEDED\_\\\ MANIFEST} as TRUE or FALSE.
% paragraph signing_and_verification (end){Signing and Verification}

\paragraph{Synchronization between video and audio}
\label{par:sync}
Since we process video and audio separately, it is a vital problem to keep them synced. Gstreamer can handle the synchronization for us in this way:

When video and audio are captured, they are timestamped by the Gstreamer. The time information will be recorded in \textit{GstBuffer} data structure which Gstreamer used to contain media data. This time information will also be transferred along with video or audio frame. Then when the consumer fetches the video or audio frame separately, the video and audio frames will be pushed into the same \textit{GstQueue}. Gstreamer will extract the timestamps hiding in the video and audio frames, then play them back together according to the timestamps.

\subsection{NDNTube}
\begin{figure*}[ht]
  \centering
  \includegraphics[scale=0.3]{ndntube_naming_pro}
  % \vspace{-0.3cm}
  \caption{NDNTube Producer and Consumer Structure}
  \label{fig:ndntube_cp}
  %\vspace{-0.2cm}
\end{figure*}
Although NDNTube is very similar to NDNLive, data production and retrieval pattern are quite different from NDNLive. We will describe them in detail in this section.

\begin{algorithm}[ht]
\caption{NDNTube producer}
\label{alg:recordproducer}
\begin{algorithmic}[3]
\State $h_v \leftarrow $ \textbf{producer}(/ndn/ucla/ndntube/video-1234/ \\\ video)
\State \textbf{setcontextopt}($h_v$, \textbf{local\_repo}, \textit{TRUE})
\vspace{0.2cm}
	\While{\textit{NOT Final Frame}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State $content_v \leftarrow $ video frame
	\State \textbf{produce}($h_v$, $Name\textbf{ }suffix_v$, $content_v$)
	%\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}
\vspace{0.2cm}
\State $h_a \leftarrow $ \textbf{producer}(/ndn/ucla/ndntube/video-1234/ \\\ audio)
\State \textbf{setcontextopt}($h_a$, \textbf{repo\_prefix}, \textit{/ndn/ucla/repo})
\vspace{0.2cm}
	\While{\textit{NOT EOF}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State $content_a \leftarrow $ audio frame
	\State \textbf{produce}($h_a$, $Name\textbf{ }suffix_a$, $content_a$)
	%\State $framenumber ++$
	\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[ht]
\caption{NDNTube consumer}
\label{alg:recordconsumer}
\begin{algorithmic}[4]
\State $h_v \leftarrow $ \textbf{consumer}(/ndn/ucla/ndntube/video-1234/ \\\ video, \textit{RDR})
%\State \textbf{setcontextopt}($h_v$, \textit{EMBEDDED\_MANIFESTS}, \textit{TRUE})
%\State \textbf{setcontextopt}($h_v$, \textbf{receive\_buffer\_size}, 1MB)
\State \textbf{setcontextopt}($h_v$, \textbf{new\_content}, \textit{ProcessVideo})
\vspace{0.2cm}
	\While{\textit{NOT EOS}}
	\State $Name \textbf{ } suffix_v \leftarrow $ video frame number
	\State \textbf{consume}($h_v$, $Name\textbf{ }suffix_v$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ProcessVideo}{byte[] \textbf{content}}
   \State $video \leftarrow $ decode \textbf{content}
%   \State $audio \leftarrow $ decode \textbf{content}
   \State Play $video$
\EndFunction

\vspace{0.4cm}

\State $h_a \leftarrow $ \textbf{consumer}(ndn/ucla/ndntube/video-1234/ \\\ audio, \textit{RDR})
\State \textbf{setcontextopt}($h_a$, \textbf{new\_content}, \textit{ProcessAudio})
\vspace{0.2cm}
	\While{\textit{NOT Final Frame}}
	\State $Name \textbf{ } suffix_a \leftarrow $ audio frame number
	\State \textbf{consume}($h_a$, $Name\textbf{ }suffix_a$)
	\State $framenumber ++$
	\EndWhile
\vspace{0.2cm}

\Function{ProcessAudio}{byte[] \textbf{content}}
%   \State $video \leftarrow $ decode \textbf{content}
   	\State $audio \leftarrow $ decode \textbf{content}
   	\State Play $audio$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Producer}

There are three producers: playlist producer, video producer and audio producer.
Playlist producer is responsible for generating the latest playlist every time it detected video file added or deleted.

Different from NDNLive, we combine content and stream\_info producer into one video or audio producer. Because for NDNLive, producers need to respond to the Interests coming from consumer directly. And content producer and stream\_info producer have different callbacks when the Interest enters the contexts or a cache\_miss is triggered, so we should separate them. But for NDNTube, all the stream\_info and content will be inserted into repo, and repo will take care of the response to consumers. There is no need to separate them. And once the producer finished producing the content and stream information, it can be offline, doesn't need to attach to the NDN Network. 

The producer side's Pseudocode is shown as Algorithm~\ref{alg:recordproducer}.

\subsubsection{Consumer}

There are five consumers: playlist consumer, video content consumer, video stream information consumer, audio content consumer and audio stream info consumer.

\paragraph{Data Retrieval Protocol} % (fold)
\label{par:ndntube_data_retrieval}

% paragraph data_retrieval (end)
Same with \textit{Streaminfo} tetrieval of NDNLive, we use \textbf{SDR} to retrieve stream information and the playing list. We want to fetch the latest version of playing list, so we should set \textit{Right\_Most\_Child} option as TRUE as well.

However, for the content retrieval, we should use \textbf{RDR} (\textit{Reliable Data Retrieval}). Because we can't stand any segment missing for the pre-recorded video, and we always want the good quality of video and audio. If the video segments are not received on time, the Interest requesting for that segment will be retransmitted. This retransmission is done by Consumer / Producer API. 

If all the retransmission failed to get the data. The consumer will resend the Interest for that frame. Such retransmission is application level. It won't send the Interest asking for the next frame until it gets the requested frame or several times application level retransmission. At the same time, when lacking of frames the \textit{Buffering} mechanism will be triggered. Only when Gstreamer accumulates enough video and audio frames (such as two seconds duration), it will continue to play back. Otherwise, it will be just paused until the buffer is full.

\paragraph{Other issues} % (fold)
\label{par:synchronization_between_video_and_audio}
There also exists the synchronization problem between video and audio. As we describe above~\ref{par:sync}, the Gstreamer will handle the synchronization part as long as we give the video and audio frame correct timestamps. In NDNLive, it is the capturing component who stamps the frames. In NDNTube, it is the \textit{Dumxer} who is responsible for time stamping. Once the media data flows through \textit{Dumxer}, this component will separate the video stream and audio stream according to their file type such as \textit{MP4} and adding the time information in each \textit{GstBuffer}.

Because all the content and stream information are all already existed and written into Repo. Then Repo takes over the responsibility. There are not \textit{NACK} in producer part. Also due to this reason, the consumer side should retrieve the data as soon as possible to keep the quality and fluency of video playing back. The default \textit{Consume\_Interval} is 0 in NDNTube.  

The consumer side's Pseudocode is shown as Algorithm~\ref{alg:recordconsumer}.
% paragraph synchronization_between_video_and_audio (end)

% section implementation (end)